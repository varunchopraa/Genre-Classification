{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tarun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/tarun/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tarun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading the input from lyrics file.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blues': 56, 'country': 86, 'disco': 45, 'hiphop': 93, 'metal': 83, 'pop': 58, 'reggae': 85, 'rock': 100}\n",
      "{'blues': 10001, 'country': 16912, 'disco': 11986, 'hiphop': 47914, 'metal': 19396, 'pop': 18786, 'reggae': 22948, 'rock': 22069}\n"
     ]
    }
   ],
   "source": [
    "stop_words = (stopwords.words('english'))\n",
    "newsw=['youve','youd','youll','shes','ive','hes','cant','never','dont','one','didnt']\n",
    "stop_words.extend(newsw)\n",
    "\n",
    "songCount = dict()\n",
    "wordCount = dict()\n",
    "\n",
    "header = ['word', 'frequency', 'genre']\n",
    "\n",
    "with open(f'Documents/genres2/freqDict.csv', 'w', newline = \"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "# a list of all the present genres\n",
    "genresList = ['blues_lyrics', 'country_lyrics', 'disco_lyrics', 'hiphop_lyrics', 'metal_lyrics', 'pop_lyrics', 'reggae_lyrics', 'rock_lyrics']\n",
    "# genresList = ['blues_lyrics', 'disco_lyrics', 'hiphop_lyrics', 'pop_lyrics', 'reggae_lyrics']\n",
    "\n",
    "# iterating over the list of all the genres in the genreList\n",
    "for genreName in genresList:\n",
    "    \n",
    "    songCount[genreName.replace('_lyrics','')] = 0\n",
    "    \n",
    "    genreLyrics = ''\n",
    "    lyricStopped = list()\n",
    "    \n",
    "    # getting the path of the current genre\n",
    "    genre = os.listdir(f'Documents/genres2/{genreName}')\n",
    "    \n",
    "    # selecting each song from each genre\n",
    "    for song in genre:\n",
    "        path = f'Documents/genres2/{genreName}/{song}'\n",
    "        \n",
    "        if song == '.DS_Store':\n",
    "            continue\n",
    "            \n",
    "        # counting songs per genre    \n",
    "        songCount[genreName.replace('_lyrics','')] += 1\n",
    "        \n",
    "        # defining the path of a song lyrics file\n",
    "        songname = open(path, 'r')\n",
    "        \n",
    "        # read all the words in lower case\n",
    "        lyric = songname.read().lower()\n",
    "        \n",
    "        # removing punctuations\n",
    "        for punct in string.punctuation:\n",
    "            lyric = lyric.replace(punct, '')\n",
    "        \n",
    "        # adding lyrics to genreLyrics list\n",
    "        genreLyrics += lyric\n",
    "    \n",
    "    # extracting words from a particular genreLyrics\n",
    "    words = genreLyrics.split()\n",
    "    \n",
    "    # counting words per genre\n",
    "    wordCount[genreName.replace('_lyrics','')] = len(words)\n",
    "    \n",
    "    # removing numbers\n",
    "    words = [x for x in words if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "    \n",
    "    # lemmatized words\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    words = [lmtzr.lemmatize(x) for x in words]\n",
    "    \n",
    "    # removing the stopwords\n",
    "    for l in words:\n",
    "        if l not in stop_words:\n",
    "            lyricStopped.append(l)\n",
    "            \n",
    "    # stemming\n",
    "    porter = PorterStemmer()\n",
    "    lyricStopped = [porter.stem(word) for word in lyricStopped]\n",
    "    \n",
    "    # frequency dictionary\n",
    "    freqDict = {}\n",
    "    \n",
    "    # counting the occurence of all the words in the dictionary\n",
    "    # no need to make a set of UNIQUE WORDS as the \"keys\" of this dictionary will all be unique\n",
    "    for word in lyricStopped:\n",
    "        freqDict[word] = freqDict.get(word, 0) + 1\n",
    "        \n",
    "    # print the dictionary\n",
    "    with open(f'Documents/genres2/freqDict.csv', 'a', newline = \"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for key, value in freqDict.items():\n",
    "            writer.writerow([key, value, genreName.replace('_lyrics','')])\n",
    "    \n",
    "    # For TF IDF: make a file containing all words in a genre (post-removing the stopwords    \n",
    "    with open(f'Documents/Genre words/{genreName}.csv', 'w', newline = \"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(lyricStopped)\n",
    "    \n",
    "            \n",
    "print(songCount)\n",
    "print(wordCount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordDict = numOfWords[A]\n",
    "# bagOfWords = bagOfWords[A]\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Data Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents is the list of NumOfWords: [numOfWordsA, numOfWordsB]\n",
    "\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val) + 1)\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfBagOfWords = tfA\n",
    "# idfs = idfs\n",
    "\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unkind</th>\n",
       "      <th>bazooka</th>\n",
       "      <th>sssssssssssssss…n</th>\n",
       "      <th>halfway</th>\n",
       "      <th>scoobi</th>\n",
       "      <th>jail</th>\n",
       "      <th>yououou</th>\n",
       "      <th>rocwild</th>\n",
       "      <th>singin</th>\n",
       "      <th>papayapa</th>\n",
       "      <th>...</th>\n",
       "      <th>murder</th>\n",
       "      <th>injuri</th>\n",
       "      <th>honeybunni</th>\n",
       "      <th>bastard</th>\n",
       "      <th>stolen</th>\n",
       "      <th>bell</th>\n",
       "      <th>huhuhuhaa</th>\n",
       "      <th>fantas</th>\n",
       "      <th>stormin</th>\n",
       "      <th>rb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 7624 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unkind   bazooka  sssssssssssssss…n   halfway    scoobi      jail  \\\n",
       "0  0.000242  0.000299           0.000000  0.000409  0.000000  0.000409   \n",
       "1  0.000000  0.000000           0.000000  0.000000  0.000000  0.000132   \n",
       "2  0.000000  0.000000           0.000000  0.000000  0.000249  0.000000   \n",
       "3  0.000050  0.000062           0.000084  0.000000  0.000123  0.000168   \n",
       "4  0.000000  0.000000           0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000136  0.000000           0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000           0.000000  0.000000  0.000000  0.000173   \n",
       "7  0.000000  0.000000           0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    yououou   rocwild    singin  papayapa  ...    murder    injuri  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000229  0.000000  ...  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000591  0.000000  ...  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000110  0.000000  ...  0.000168  0.000084   \n",
       "4  0.000000  0.000000  0.000000  0.000000  ...  0.000213  0.000000   \n",
       "5  0.000000  0.000688  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "6  0.000345  0.000000  0.001275  0.000173  ...  0.007850  0.000000   \n",
       "7  0.000000  0.000000  0.000328  0.000000  ...  0.002449  0.000000   \n",
       "\n",
       "   honeybunni   bastard    stolen      bell  huhuhuhaa    fantas   stormin  \\\n",
       "0    0.000000  0.000000  0.000000  0.000356   0.000000  0.000000  0.000000   \n",
       "1    0.000263  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  0.000148   0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.000062  0.000149  0.000330   0.000084  0.000000  0.000084   \n",
       "4    0.000000  0.000312  0.000378  0.000926   0.000000  0.000000  0.000000   \n",
       "5    0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "6    0.000000  0.000000  0.000204  0.000000   0.000000  0.000000  0.000000   \n",
       "7    0.000000  0.000000  0.000000  0.000164   0.000000  0.000565  0.000000   \n",
       "\n",
       "         rb  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.000000  \n",
       "3  0.000185  \n",
       "4  0.000000  \n",
       "5  0.000000  \n",
       "6  0.000379  \n",
       "7  0.000000  \n",
       "\n",
       "[8 rows x 7624 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genresList = ['blues_lyrics', 'country_lyrics', 'disco_lyrics', 'hiphop_lyrics', 'metal_lyrics', 'pop_lyrics', 'reggae_lyrics', 'rock_lyrics']\n",
    "\n",
    "genres = os.listdir(f'Documents/Genre words')\n",
    "BOW = {} # bag of words dictionary(of list)\n",
    "\n",
    "for genre in genres:\n",
    "    path = f'Documents/Genre words/{genre}'\n",
    "    wordsFile = open(path, 'r')\n",
    "    BOW[genre.replace('.csv', '')] = wordsFile.read().split(',')\n",
    "\n",
    "UniqueWords = set(BOW['blues_lyrics']).union(set(BOW['country_lyrics']).union(set(BOW['disco_lyrics']).union(set(BOW['hiphop_lyrics']).union(set(BOW['metal_lyrics']).union(set(BOW['pop_lyrics']).union(set(BOW['reggae_lyrics']).union(set(BOW['rock_lyrics']))))))))\n",
    "# print(UniqueWords)\n",
    "\n",
    "# number of words dictionary\n",
    "numOfWords = {}\n",
    "tf = {}\n",
    "numOfWordsList = list()\n",
    "\n",
    "for genreName in genresList:\n",
    "    numOfWords[genreName] = dict.fromkeys(UniqueWords, 0)\n",
    "    temp = numOfWords[genreName]\n",
    "\n",
    "    # updating the count of each word\n",
    "    for word in BOW[genreName]:\n",
    "        temp[word] += 1\n",
    "\n",
    "    numOfWords[genreName] = temp\n",
    "\n",
    "    # Term Frequency\n",
    "    tf[genreName] = computeTF(numOfWords[genreName], BOW[genreName])\n",
    "    \n",
    "    # updating numOfWordsList\n",
    "    numOfWordsList.append(numOfWords[genreName])\n",
    "    \n",
    "# computing IDF\n",
    "idfs = computeIDF(numOfWordsList)\n",
    "\n",
    "# computing TF IDF\n",
    "tfidf = {}\n",
    "tfidfList = list()\n",
    "\n",
    "for genreName in genresList:\n",
    "    tfidf[genreName] = computeTFIDF(tf[genreName], idfs)\n",
    "    tfidfList.append(tfidf[genreName])\n",
    "\n",
    "# pandas dataframe\n",
    "df = pd.DataFrame(tfidfList)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cold' 4]\n",
      " ['summer' 1]\n",
      " ['night' 43]\n",
      " ...\n",
      " ['older' 2]\n",
      " ['cowboy' 2]\n",
      " ['ridin' 1]]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Documents/genres2/freqDict.csv')\n",
    "X = dataset.iloc[:, [0,1]].values\n",
    "y = dataset.iloc[:, 2].values\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "dataset.groupby('genre').frequency.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15440, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=X[:,0]\n",
    "frequency=X[:,1]\n",
    "frequency= np.reshape(frequency,(-1,1))\n",
    "frequency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15440, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "word_encoded=le.fit_transform(word.astype(str))\n",
    "word_encoded = np.reshape(word_encoded,(-1,1))\n",
    "word_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15440, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.column_stack((word_encoded,frequency))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = 0.2, random_state = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 247   0   0   0   0]\n",
      " [  0   0   0 307   0   0   0   0]\n",
      " [  0   0   0 215   0   1   0   0]\n",
      " [  0   0   0 922   0   6   0   0]\n",
      " [  0   0   0 376   0   0   0   0]\n",
      " [  0   0   0 254   0   2   0   0]\n",
      " [  0   0   0 352   0   1   0   0]\n",
      " [  0   0   0 403   0   2   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29922279792746115\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 247   0   0   0   0]\n",
      " [  0   0   0 307   0   0   0   0]\n",
      " [  0   0   0 216   0   0   0   0]\n",
      " [  0   0   0 928   0   0   0   0]\n",
      " [  0   0   0 376   0   0   0   0]\n",
      " [  0   0   0 256   0   0   0   0]\n",
      " [  0   0   0 353   0   0   0   0]\n",
      " [  0   0   0 405   0   0   0   0]]\n",
      "Accuracy: 0.3005181347150259\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 42)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38  34  12  90  27   8  20  18]\n",
      " [ 45  43  13 132  23  11  11  29]\n",
      " [ 36  33  11  83  18   4  17  14]\n",
      " [107 115  48 419  79  31  65  64]\n",
      " [ 53  40  19 164  32  13  27  28]\n",
      " [ 26  40  15 101  32  16  17   9]\n",
      " [ 39  52  22 140  36  16  31  17]\n",
      " [ 56  48  20 169  32  16  40  24]]\n",
      "Accuracy: 0.19883419689119172\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  21  20  82  40  17  25  32]\n",
      " [ 28  21  23  93  50  17  33  42]\n",
      " [ 21  24  15  63  28  18  21  26]\n",
      " [ 61  83  56 317 122  69  95 125]\n",
      " [ 27  50  27 108  32  27  44  61]\n",
      " [ 19  36  24  77  42  18  23  17]\n",
      " [ 16  31  24 119  51  32  38  42]\n",
      " [ 34  51  35 131  56  28  36  34]]\n",
      "Accuracy: 0.15705958549222798\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 247   0   0   0   0]\n",
      " [  0   0   0 307   0   0   0   0]\n",
      " [  0   0   0 216   0   0   0   0]\n",
      " [  0   0   0 928   0   0   0   0]\n",
      " [  0   0   0 376   0   0   0   0]\n",
      " [  0   0   0 256   0   0   0   0]\n",
      " [  0   0   0 353   0   0   0   0]\n",
      " [  0   0   0 405   0   0   0   0]]\n",
      "Accuracy: 0.3005181347150259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# NN\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29954663212435234\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NN?\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras import layers\n",
    "\n",
    "# input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(layers.Dense(512, input_dim=input_dim, activation='relu'))\n",
    "\n",
    "# model.add(layers.Dense(256, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "\n",
    "# # y_train = encoder.fit_transform(y_train)\n",
    "# # y_test = encoder.fit_transform(y_test)\n",
    "# y_enc = encoder.fit_transform(y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, y_enc, test_size = 0.2, random_state = 15)\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=20, batch_size=256)\n",
    "\n",
    "# # loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "# # print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "# # loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "# # print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "# # calculate accuracy\n",
    "# test_loss, test_acc = model.evaluate(X_test,y_test)\n",
    "# print('test_acc: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-28533edfc3e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(64, activation='sigmoid'))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(X_train,y_train,epochs=20,batch_size=64)\n",
    "\n",
    "# calculate accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test,y_test)\n",
    "print('test_acc: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
